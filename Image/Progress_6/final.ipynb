{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face 1: neutral with probability 0.8946856260299683\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Define an emotion classification model\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):  # Assuming there are 7 emotions to classify\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        # Use a pre-trained Densenet121 model\n",
    "        self.densenet = models.densenet121(pretrained=True)\n",
    "        # Replace the classifier layer with a linear layer for our specific task\n",
    "        num_ftrs = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass of the model\n",
    "        return self.densenet(x)\n",
    "\n",
    "# Load the model\n",
    "model = EmotionClassifier().to('cuda')  # Move model to CUDA (GPU)\n",
    "model.load_state_dict(torch.load('best_model.pt'))  # Load the best saved model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define preprocessing for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize images to 256x256\n",
    "    transforms.CenterCrop(224),  # Crop the center 224x224 portion of the image\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet's mean and std\n",
    "])\n",
    "\n",
    "# Function to predict emotion of a face\n",
    "def predict_emotion(model, face_img):\n",
    "    image_tensor = transform(face_img).unsqueeze(0).to('cuda')  # Preprocess and move to CUDA\n",
    "    with torch.no_grad():  # No gradient calculation for inference\n",
    "        outputs = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        return probabilities.cpu().numpy()  # Return probabilities as a numpy array\n",
    "\n",
    "# Detect and crop face from an image\n",
    "def detect_and_crop_face(image_path):\n",
    "    mtcnn = MTCNN(keep_all=True, device='cuda')  # Initialize MTCNN for face detection\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "    boxes, _ = mtcnn.detect(img)  # Detect faces\n",
    "    if boxes is not None and len(boxes) > 0:\n",
    "        # Calculate area for each detected face\n",
    "        areas = [(box[2]-box[0])*(box[3]-box[1]) for box in boxes]\n",
    "        # Find the largest face\n",
    "        max_area_idx = np.argmax(areas)\n",
    "        box = boxes[max_area_idx].astype(int)\n",
    "        # Crop the largest face\n",
    "        cropped_face = img[box[1]:box[3], box[0]:box[2]]\n",
    "        return [cropped_face]  # Return the cropped face as a list for consistency in later processing\n",
    "    return None\n",
    "\n",
    "\n",
    "# Integration process\n",
    "image_path = '5.png'  # Replace with your image path\n",
    "cropped_faces = detect_and_crop_face(image_path)\n",
    "emotion_labels = ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "if cropped_faces is not None:\n",
    "    for i, face in enumerate(cropped_faces):\n",
    "        face_img = Image.fromarray(face)  # Convert array to PIL Image\n",
    "        probabilities = predict_emotion(model, face_img)\n",
    "        predicted_emotion = emotion_labels[probabilities.argmax()]  # Get the highest probability emotion\n",
    "        print(f\"Face {i+1}: {predicted_emotion} with probability {probabilities.max()}\")\n",
    "        # Optionally display or save the cropped face and its predicted emotion\n",
    "        # face_img.show()\n",
    "        # face_img.save(f'cropped_face_{i}_{predicted_emotion}.png')\n",
    "else:\n",
    "    print(\"No faces were detected.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
