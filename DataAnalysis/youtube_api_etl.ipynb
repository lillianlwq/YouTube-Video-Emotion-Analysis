{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from private import DEVELOPER_KEY\n",
    "from googleapiclient.discovery import build\n",
    "from pprint import pprint\n",
    "from emotion_text_classifier import analyze_sentiment\n",
    "import pandas as pd\n",
    "import csv\n",
    "from langdetect import detect\n",
    "from pyspark.sql import SparkSession, functions, types, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements:\n",
    "# pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Build YouTube API ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = build(api_service_name, api_version, developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get video_id from all_audios_emotion.csv\n",
    "video_ids = pd.read_csv('all_audios_emotion.csv')['video_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = youtube.videos().list(\n",
    "    part = ['statistics', 'snippet'],\n",
    "    id = video_ids\n",
    ")    \n",
    "response = request.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for item in response['items']:\n",
    "    statisitcs = item['statistics']\n",
    "    statisitcs.pop('favoriteCount', None)\n",
    "    # tags to string\n",
    "    tags = item['snippet'].get('tags', [])\n",
    "    tags = ', '.join(tags)\n",
    "    statisitcs.update({'tags': tags})\n",
    "    contents.append(statisitcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the comment is in English\n",
    "def is_english(s):\n",
    "    try:\n",
    "        return detect(s) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def parse_sentiment(sentiment_list):\n",
    "    sentiment_sums = {}\n",
    "    sentiment_counts = {}\n",
    "\n",
    "    for sentiment_str in sentiment_list:\n",
    "        sentiments = sentiment_str.split(\"\\n\")\n",
    "        for sentiment in sentiments:\n",
    "            sentiment_name, sentiment_value = sentiment.split(\" : \")\n",
    "            sentiment_value = float(sentiment_value.strip(\"%\"))\n",
    "            sentiment_sums[sentiment_name] = sentiment_sums.get(sentiment_name, 0) + sentiment_value\n",
    "            sentiment_counts[sentiment_name] = sentiment_counts.get(sentiment_name, 0) + 1\n",
    "\n",
    "    sentiment_averages = {sentiment: sentiment_sums[sentiment] / sentiment_counts[sentiment] for sentiment in sentiment_sums}\n",
    "    return sentiment_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{' anger': 24.14157894736842, ' neutral': 34.918000000000006, ' sadness': 8.57375, ' joy': 60.6912, ' surprise': 6.186, ' disgust': 3.0999999999999996, ' fear': 4.725}, {' anger': 16.611428571428572, ' surprise': 21.21619047619047, ' disgust': 8.8125, ' neutral': 48.231351351351364, ' joy': 43.95172413793104, ' sadness': 15.193999999999999, ' fear': 6.4825}, {' anger': 15.55136363636364, ' surprise': 13.644375, ' joy': 32.865, ' neutral': 54.125, ' sadness': 33.400666666666666, ' fear': 9.487499999999999, ' disgust': 8.778}, {' joy': 53.646999999999984, ' neutral': 34.44842105263158, ' anger': 6.503478260869565, ' surprise': 17.156000000000002, ' fear': 40.66, ' sadness': 30.954615384615384, ' disgust': 49.07500000000001}, {' neutral': 40.12250000000001, ' disgust': 7.099999999999999, ' sadness': 2.068888888888889, ' joy': 64.51037037037035, ' anger': 12.133529411764707, ' surprise': 20.912222222222223, ' fear': 1.5859999999999999}]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Get youtube video comments\n",
    "average_sentiments = []\n",
    "for video_id in video_ids:\n",
    "    request = youtube.commentThreads().list(\n",
    "        part = 'snippet',\n",
    "        videoId = video_id,\n",
    "        maxResults = 50\n",
    "    )\n",
    "    response = request.execute()\n",
    "    comments_temp = []\n",
    "    for item in response['items']:\n",
    "        # Navigate through the nested dictionaries to get the comment text\n",
    "        comment = item['snippet']['topLevelComment']['snippet']\n",
    "        # Check if the comment is english and length is less than 100\n",
    "        if is_english(comment['textDisplay']) and len(comment['textDisplay']) < 500:\n",
    "            comments_temp.append(comment['textDisplay'])\n",
    "    avg_sentiment = parse_sentiment([analyze_sentiment(comment) for comment in comments_temp])\n",
    "    average_sentiments.append(avg_sentiment)\n",
    "\n",
    "# Print the 5 comments\n",
    "print(average_sentiments[:5])\n",
    "print(len(average_sentiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert joy to happiness\n",
    "for sentiment in average_sentiments:\n",
    "    sentiment[' happiness'] = sentiment.pop(' joy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The sentiment analysis model is trained on BERT, which typically has a maximum input length of 512 tokens. So I filtered out the comments that are longer than 500 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the average sentiment values\n",
    "average_sentiments_dict = {sentiment: [sentiment_dict[sentiment] for sentiment_dict in average_sentiments] for sentiment in average_sentiments[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a header\n",
    "header = ['video_id', 'viewCount', 'likeCount', 'commentCount', 'tags'] + [\"comment_\" + key.strip() for key in list(average_sentiments[0].keys())]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('youtube_contents_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # Iterate over each content item and its index\n",
    "    for i, content in enumerate(contents):\n",
    "        # Prepare the tags string\n",
    "        # Fetch the corresponding sentiment values by index\n",
    "        sentiment_values = [average_sentiments_dict[sentiment][i] for sentiment in average_sentiments[0].keys()]\n",
    "        # Write the row to the CSV\n",
    "        writer.writerow([\n",
    "            video_ids[i],\n",
    "            content['viewCount'], content['likeCount'], content['commentCount'], content['tags']\n",
    "        ] + sentiment_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (v3.10.0:b494f5935c, Oct  4 2021, 14:59:19) [Clang 12.0.5 (clang-1205.0.22.11)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
